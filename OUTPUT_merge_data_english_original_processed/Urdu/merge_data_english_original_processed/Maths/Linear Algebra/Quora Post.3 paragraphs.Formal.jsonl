{"unnormalised": "مجھ سے کسی نے مشین لرننگ میں لکیری الجبرا کے اطلاقات کے بارے میں پوچھا۔ دراصل، یہ کافی بنیادی ہے۔ ایک بنیادی نیورل نیٹ ورک لیئر پر غور کریں؛ یہ بنیادی طور پر میٹرکس ضرب ہے۔ ہر نیوران کا آؤٹ پٹ اپنے ان پُٹس کا وزنی مجموعہ ہوتا ہے، اور ان وزنوں کو میٹرکس میں منظم کیا جاتا ہے۔ مثال کے طور پر، ایک لیئر کے وزن W ابعاد (100 x 50) کے ساتھ ہو سکتے ہیں، جس کا مطلب ہے کہ یہ 50 جہتی ان پُٹ کو 100 جہتی آؤٹ پُٹ میں تبدیل کرتا ہے۔ فارورڈ پاس میں Wx + b کا حساب لگانا شامل ہے، جہاں x ان پُٹ ویکٹر ہے اور b ایک تعصب (bias) ویکٹر ہے۔ یہ 'Wx' بالکل وہ جگہ ہے جہاں لکیری الجبرا چمکتا ہے۔", "normalised": "مجھ سے کسی نے مشین لرننگ میں لینیئر الجبرا کی ایپلی کیشنز کے بارے میں پوچھا۔ ٹھیک ہے، یہ کافی بنیادی ہے۔ ایک بنیادی نیورل نیٹ ورک لیئر پر غور کریں؛ یہ بنیادی طور پر میٹرکس ملٹیپلیکیشن ہے۔ ہر نیورون کا آؤٹ پٹ اس کے ان پُٹس کا ویٹڈ سم ہوتا ہے، اور ان ویٹس کو میٹرکس میں ترتیب دیا جاتا ہے۔ مثال کے طور پر، ایک لیئر میں وزن W ہو سکتا ہے جس کی ڈائمینشن ایک سو بائی پچاس ہو، یعنی یہ پچاس ڈائمینشنل ان پُٹ کو ایک سو ڈائمینشنل آؤٹ پُٹ میں تبدیل کرتا ہے۔ فارورڈ پاس میں W x جمع b کا حساب لگانا شامل ہے، جہاں x ان پُٹ ویکٹر ہے اور b ایک بائیس ویکٹر ہے۔ یہ 'W x' بالکل وہی جگہ ہے جہاں لینیئر الجبرا چمکتا ہے۔", "text": "مجھ سے کسی نے مشین لرننگ میں لکیری الجبرا کے اطلاقات کے بارے میں پوچھا۔ دراصل، یہ کافی بنیادی ہے۔ ایک بنیادی نیورل نیٹ ورک لیئر پر غور کریں؛ یہ بنیادی طور پر میٹرکس ضرب ہے۔ ہر نیوران کا آؤٹ پٹ اپنے ان پُٹس کا وزنی مجموعہ ہوتا ہے، اور ان وزنوں کو میٹرکس میں منظم کیا جاتا ہے۔ مثال کے طور پر، ایک لیئر کے وزن W ابعاد (100 x 50) کے ساتھ ہو سکتے ہیں، جس کا مطلب ہے کہ یہ 50 جہتی ان پُٹ کو 100 جہتی آؤٹ پُٹ میں تبدیل کرتا ہے۔ فارورڈ پاس میں Wx + b کا حساب لگانا شامل ہے، جہاں x ان پُٹ ویکٹر ہے اور b ایک تعصب (bias) ویکٹر ہے۔ یہ 'Wx' بالکل وہ جگہ ہے جہاں لکیری الجبرا چمکتا ہے۔"}
{"unnormalised": "پھر آپ کے پاس امیج پروسیسنگ ہے۔ ایک تصویر کو میٹرکس (یا رنگین تصاویر کے لیے میٹرکس کے سیٹ) کے طور پر پیش کیا جا سکتا ہے۔ دھندلا کرنا، ایج کا پتہ لگانا، اور گردش جیسے آپریشن لکیری الجبرا کے تصورات پر انحصار کرتے ہیں۔ مثال کے طور پر، 3x3 کنوولوشن کرنل کا اطلاق عنصری ضرب اور جمع پر مشتمل ہوتا ہے جسے میٹرکس آپریشنز کا استعمال کرتے ہوئے پیش کیا جا سکتا ہے۔ یہاں تک کہ کسی تصویر کا سائز تبدیل کرنے یا اسے تراشنے جیسے سادہ کاموں کو بھی لکیری ٹرانسفارمیشن کے طور پر دیکھا جا سکتا ہے۔ امیج کمپریشن کی تکنیکیں جیسے کہ PCA بھی لکیری الجبرا کا بہت زیادہ استعمال کرتی ہیں۔", "normalised": "پھر آپ کے پاس امیج پروسیسنگ ہے۔ ایک تصویر کو میٹرکس یا رنگین تصاویر کے لیے میٹرکس کے ایک سیٹ کے طور پر پیش کیا جا سکتا ہے۔ بلر کرنا، ایج ڈیٹیکشن، اور روٹیشن جیسے آپریشن لکیری الجبرا کے تصورات پر انحصار کرتے ہیں۔ مثال کے طور پر، ایک تین بائی تین کنوولوشن کرنل کا اطلاق عنصر وار ضرب اور جمع پر مشتمل ہوتا ہے جسے میٹرکس آپریشنز کا استعمال کرتے ہوئے پیش کیا جا سکتا ہے۔ یہاں تک کہ تصویر کو ری سائز کرنے یا اسے کراپ کرنے جیسے سادہ کاموں کو بھی لکیری تبدیلیوں کے طور پر دیکھا جا سکتا ہے۔ امیج کمپریشن کی تکنیکیں جیسے پی-سی-اے بھی لکیری الجبرا کا بکثرت استعمال کرتی ہیں۔", "text": "پھر آپ کے پاس امیج پروسیسنگ ہے۔ ایک تصویر کو میٹرکس (یا رنگین تصاویر کے لیے میٹرکس کے سیٹ) کے طور پر پیش کیا جا سکتا ہے۔ دھندلا کرنا، ایج کا پتہ لگانا، اور گردش جیسے آپریشن لکیری الجبرا کے تصورات پر انحصار کرتے ہیں۔ مثال کے طور پر، 3x3 کنوولوشن کرنل کا اطلاق عنصری ضرب اور جمع پر مشتمل ہوتا ہے جسے میٹرکس آپریشنز کا استعمال کرتے ہوئے پیش کیا جا سکتا ہے۔ یہاں تک کہ کسی تصویر کا سائز تبدیل کرنے یا اسے تراشنے جیسے سادہ کاموں کو بھی لکیری ٹرانسفارمیشن کے طور پر دیکھا جا سکتا ہے۔ امیج کمپریشن کی تکنیکیں جیسے کہ PCA بھی لکیری الجبرا کا بہت زیادہ استعمال کرتی ہیں۔"}
{"unnormalised": "مزید یہ کہ، لکیری الجبرا (Linear Algebra) مشین لرننگ (Machine Learning) میں استعمال ہونے والے کئی آپٹیمائزیشن الگورتھمز (optimization algorithms) جیسے گریڈیئنٹ ڈیسنٹ (Gradient Descent) کی بنیاد ہے۔ یہ الگورتھمز (algorithms) مؤثر طریقے سے ماڈلز کی تربیت کے لیے بہت اہم ہیں۔ لہذا، اگرچہ آپ ہر وقت واضح طور پر میٹرکس (matrices) نہیں لکھ رہے ہوں گے، لیکن لکیری الجبرا (linear algebra) کی مضبوط سمجھ یقیناً آپ کو بہت سے ایم ایل (ML) ماڈلز کے اندرونی کام کو سمجھنے میں مدد دے گی، اور آپ کے کوڈ کو ڈیبگ (debug) کرنے میں بھی مدد کرے گی۔ آپ کو ہر وقت آئیگن ویلیوز (eigenvalues)، آئیگن ویکٹرز (eigenvectors)، میٹرکس ڈی کمپوزیشن (matrix decomposition) اور ویکٹر اسپیسز (vector spaces) جیسی چیزوں کا سامنا رہے گا۔", "normalised": "مزید یہ کہ لکیری الجبرا (Linear Algebra) مشین لرننگ میں استعمال ہونے والے کئی آپٹیمائزیشن الگورتھمز (optimization algorithms) جیسے گریڈینٹ ڈیسنٹ (Gradient Descent) کی بنیاد ہے۔ یہ الگورتھمز ماڈلز کو مؤثر طریقے سے تربیت دینے کے لیے بہت اہم ہیں۔ لہذا، اگرچہ آپ ہر وقت واضح طور پر میٹرکس (matrices) نہ بھی لکھ رہے ہوں، لیکن لکیری الجبرا کی مضبوط سمجھ یقینی طور پر آپ کو بہت سے ایم ایل (M-L) ماڈلز کے اندرونی کام کو سمجھنے میں مدد کرے گی، اور آپ کے کوڈ کو ڈیبگ (debugging) کرنے میں بھی مدد کرے گی۔ آپ کو ہر وقت آئیگن ویلیوز (eigenvalues)، آئیگن ویکٹرز (eigenvectors)، میٹرکس ڈی کمپوزیشن (matrix decomposition)، اور ویکٹر سپیسز (vector spaces) جیسی چیزوں کا سامنا رہے گا۔", "text": "مزید یہ کہ، لکیری الجبرا (Linear Algebra) مشین لرننگ (Machine Learning) میں استعمال ہونے والے کئی آپٹیمائزیشن الگورتھمز (optimization algorithms) جیسے گریڈیئنٹ ڈیسنٹ (Gradient Descent) کی بنیاد ہے۔ یہ الگورتھمز (algorithms) مؤثر طریقے سے ماڈلز کی تربیت کے لیے بہت اہم ہیں۔ لہذا، اگرچہ آپ ہر وقت واضح طور پر میٹرکس (matrices) نہیں لکھ رہے ہوں گے، لیکن لکیری الجبرا (linear algebra) کی مضبوط سمجھ یقیناً آپ کو بہت سے ایم ایل (ML) ماڈلز کے اندرونی کام کو سمجھنے میں مدد دے گی، اور آپ کے کوڈ کو ڈیبگ (debug) کرنے میں بھی مدد کرے گی۔ آپ کو ہر وقت آئیگن ویلیوز (eigenvalues)، آئیگن ویکٹرز (eigenvectors)، میٹرکس ڈی کمپوزیشن (matrix decomposition) اور ویکٹر اسپیسز (vector spaces) جیسی چیزوں کا سامنا رہے گا۔"}
