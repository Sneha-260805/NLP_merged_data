```json
{
  "unnormalised": "किसी ने मुझसे मशीन लर्निंग में लीनियर अलजेब्रा के अनुप्रयोगों के बारे में पूछा। खैर, यह बहुत बुनियादी है। एक बुनियादी न्यूरल नेटवर्क परत पर विचार करें; यह अनिवार्य रूप से एक मैट्रिक्स गुणन है। प्रत्येक न्यूरॉन का आउटपुट इसके इनपुट का भारित योग है, और इन भारों को मैट्रिक्स में व्यवस्थित किया जाता है। उदाहरण के लिए, एक परत में आयाम (100 x 50) के साथ भार W हो सकता है, जिसका अर्थ है कि यह एक 50-आयामी इनपुट को 100-आयामी आउटपुट में बदल देता है। फॉरवर्ड पास में Wx + b की गणना करना शामिल है, जहाँ x इनपुट वेक्टर है और b एक बायस वेक्टर है। यह 'Wx' ठीक वही है जहाँ लीनियर अलजेब्रा चमकता है।",
  "normalised": "किसी ने मुझसे मशीन लर्निंग में लीनियर अलजेब्रा के अनुप्रयोगों के बारे में पूछा। खैर, यह बहुत बुनियादी है। एक बुनियादी न्यूरल नेटवर्क परत पर विचार करें; यह अनिवार्य रूप से एक मैट्रिक्स गुणन है। प्रत्येक न्यूरॉन का आउटपुट इसके इनपुट का भारित योग है, और इन भारों को मैट्रिक्स में व्यवस्थित किया जाता है। उदाहरण के लिए, एक परत में एक सौ गुणा पचास आयामों के साथ भार W हो सकता है, जिसका अर्थ है कि यह एक पचास-आयामी इनपुट को एक सौ-आयामी आउटपुट में बदल देता है। फॉरवर्ड पास में W x प्लस b की गणना करना शामिल है, जहाँ x इनपुट वेक्टर है और b एक बायस वेक्टर है। यह 'W x' ठीक वही है जहाँ लीनियर अलजेब्रा चमकता है।",
  "raw": "किसी ने मुझसे मशीन लर्निंग में लीनियर अलजेब्रा के अनुप्रयोगों के बारे में पूछा। खैर, यह बहुत बुनियादी है। एक बुनियादी न्यूरल नेटवर्क परत पर विचार करें; यह अनिवार्य रूप से एक मैट्रिक्स गुणन है। प्रत्येक न्यूरॉन का आउटपुट इसके इनपुट का भारित योग है, और इन भारों को मैट्रिक्स में व्यवस्थित किया जाता है। उदाहरण के लिए, एक परत में आयाम (100 x 50) के साथ भार W हो सकता है, जिसका अर्थ है कि यह एक 50-आयामी इनपुट को 100-आयामी आउटपुट में बदल देता है। फॉरवर्ड पास में Wx + b की गणना करना शामिल है, जहाँ x इनपुट वेक्टर है और b एक बायस वेक्टर है। यह 'Wx' ठीक वही है जहाँ लीनियर अलजेब्रा चमकता है।"
}
```
```json
{
  "unnormalised": "फिर आपके पास इमेज प्रोसेसिंग है। एक इमेज को एक मैट्रिक्स (या रंगीन इमेज के लिए मैट्रिक्स का एक सेट) के रूप में दर्शाया जा सकता है। धुंधला करने, किनारे का पता लगाने और घुमाने जैसे ऑपरेशन सभी रैखिक बीजगणित अवधारणाओं पर निर्भर करते हैं। उदाहरण के लिए, 3x3 कनवल्शन कर्नेल को लागू करने में तत्व-वार गुणन और संकलन शामिल है जिसे मैट्रिक्स संचालन का उपयोग करके दर्शाया जा सकता है। यहां तक कि इमेज का आकार बदलने या उसे क्रॉप करने जैसे सरल कार्यों को भी रैखिक परिवर्तनों के रूप में देखा जा सकता है। पीसीए जैसी इमेज कंप्रेशन तकनीकें भी रैखिक बीजगणित का भारी उपयोग करती हैं।",
  "normalised": "फिर आपके पास इमेज प्रोसेसिंग है। एक इमेज को एक मैट्रिक्स या रंगीन इमेज के लिए मैट्रिक्स का एक सेट के रूप में दर्शाया जा सकता है। धुंधला करने, किनारे का पता लगाने और घुमाने जैसे ऑपरेशन सभी रैखिक बीजगणित अवधारणाओं पर निर्भर करते हैं। उदाहरण के लिए, तीन गुणा तीन कनवल्शन कर्नेल को लागू करने में तत्व-वार गुणन और संकलन शामिल है जिसे मैट्रिक्स संचालन का उपयोग करके दर्शाया जा सकता है। यहां तक कि इमेज का आकार बदलने या उसे क्रॉप करने जैसे सरल कार्यों को भी रैखिक परिवर्तनों के रूप में देखा जा सकता है। पी-सी-ए जैसी इमेज कंप्रेशन तकनीकें भी रैखिक बीजगणित का भारी उपयोग करती हैं।",
  "raw": "फिर आपके पास इमेज प्रोसेसिंग है। एक इमेज को एक मैट्रिक्स (या रंगीन इमेज के लिए मैट्रिक्स का एक सेट) के रूप में दर्शाया जा सकता है। धुंधला करने, किनारे का पता लगाने और घुमाने जैसे ऑपरेशन सभी रैखिक बीजगणित अवधारणाओं पर निर्भर करते हैं। उदाहरण के लिए, 3x3 कनवल्शन कर्नेल को लागू करने में तत्व-वार गुणन और संकलन शामिल है जिसे मैट्रिक्स संचालन का उपयोग करके दर्शाया जा सकता है। यहां तक कि इमेज का आकार बदलने या उसे क्रॉप करने जैसे सरल कार्यों को भी रैखिक परिवर्तनों के रूप में देखा जा सकता है। पीसीए जैसी इमेज कंप्रेशन तकनीकें भी रैखिक बीजगणित का भारी उपयोग करती हैं।"
}
```
```json
{
  "unnormalised": "इसके अलावा, लीनियर अलजेब्रा, मशीन लर्निंग में उपयोग किए जाने वाले कई ऑप्टिमाइजेशन एल्गोरिदम जैसे ग्रेडिएंट डिसेंट को रेखांकित करता है। ये एल्गोरिदम मॉडल को कुशलतापूर्वक प्रशिक्षित करने के लिए महत्वपूर्ण हैं। तो, भले ही आप हर समय स्पष्ट रूप से मैट्रिक्स न लिख रहे हों, लीनियर अलजेब्रा की एक मजबूत समझ निश्चित रूप से आपको कई एमएल मॉडलों के आंतरिक कामकाज को समझने में मदद करेगी, और आपके कोड को डीबग करने में भी मदद करेगी। आपको हर समय eigenvalues, eigenvectors, मैट्रिक्स डिकम्पोजिशन और वेक्टर स्पेस जैसी चीजें मिलेंगी।",
  "normalised": "इसके अलावा, लीनियर अलजेब्रा, मशीन लर्निंग में उपयोग किए जाने वाले कई ऑप्टिमाइजेशन एल्गोरिदम जैसे ग्रेडिएंट डिसेंट को रेखांकित करता है। ये एल्गोरिदम मॉडल को कुशलतापूर्वक प्रशिक्षित करने के लिए महत्वपूर्ण हैं। तो, भले ही आप हर समय स्पष्ट रूप से मैट्रिक्स न लिख रहे हों, लीनियर अलजेब्रा की एक मजबूत समझ निश्चित रूप से आपको कई एमएल मॉडलों के आंतरिक कामकाज को समझने में मदद करेगी, और आपके कोड को डीबग करने में भी मदद करेगी। आपको हर समय eigenvalues, eigenvectors, मैट्रिक्स डिकम्पोजिशन और वेक्टर स्पेस जैसी चीजें मिलेंगी।",
  "raw": "इसके अलावा, लीनियर अलजेब्रा, मशीन लर्निंग में उपयोग किए जाने वाले कई ऑप्टिमाइजेशन एल्गोरिदम जैसे ग्रेडिएंट डिसेंट को रेखांकित करता है। ये एल्गोरिदम मॉडल को कुशलतापूर्वक प्रशिक्षित करने के लिए महत्वपूर्ण हैं। तो, भले ही आप हर समय स्पष्ट रूप से मैट्रिक्स न लिख रहे हों, लीनियर अलजेब्रा की एक मजबूत समझ निश्चित रूप से आपको कई एमएल मॉडलों के आंतरिक कामकाज को समझने में मदद करेगी, और आपके कोड को डीबग करने में भी मदद करेगी। आपको हर समय eigenvalues, eigenvectors, मैट्रिक्स डिकम्पोजिशन और वेक्टर स्पेस जैसी चीजें मिलेंगी।"
}
```
