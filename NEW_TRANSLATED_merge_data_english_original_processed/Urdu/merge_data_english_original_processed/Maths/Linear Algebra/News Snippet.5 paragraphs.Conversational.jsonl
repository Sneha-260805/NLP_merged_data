```json
{
  "unnormalised": "آئی آئی ٹی دہلی کے محققین نے بظاہر لکیری الجبرا کے مسائل کو حل کرنے کا ایک نیا طریقہ دریافت کیا ہے۔ ایسا لگتا ہے کہ وہ اسے \"Modified Gauss-Jordan Elimination++\" کہہ رہے ہیں – فینسی، ہے نا؟ بظاہر، یہ 1000x1000 سے بڑے میٹرکس کے لیے بہت تیز ہونا چاہیے، جو کہ AI اور بگ ڈیٹا اینالیٹکس جیسے شعبوں میں ایک بڑی بات ہے۔",
  "normalised": "آئی-آئی-ٹی دہلی کے محققین نے بظاہر لکیری الجبرا کے مسائل کو حل کرنے کا ایک نیا طریقہ دریافت کیا ہے۔ ایسا لگتا ہے کہ وہ اسے \"Modified Gauss-Jordan Elimination plus plus\" کہہ رہے ہیں – فینسی، ہے نا؟ بظاہر، یہ ایک ہزار بائے ایک ہزار سے بڑے میٹرکس کے لیے بہت تیز ہونا چاہیے، جو کہ اے-آئی اور بگ ڈیٹا اینالیٹکس جیسے شعبوں میں ایک بڑی بات ہے۔",
  "raw": "آئی آئی ٹی دہلی کے محققین نے بظاہر لکیری الجبرا کے مسائل کو حل کرنے کا ایک نیا طریقہ دریافت کیا ہے۔ ایسا لگتا ہے کہ وہ اسے \"Modified Gauss-Jordan Elimination++\" کہہ رہے ہیں – فینسی، ہے نا؟ بظاہر، یہ 1000x1000 سے بڑے میٹرکس کے لیے بہت تیز ہونا چاہیے، جو کہ AI اور بگ ڈیٹا اینالیٹکس جیسے شعبوں میں ایک بڑی بات ہے۔"
}
```
```json
{
  "unnormalised": "ٹیم، جس کی قیادت پروفیسر مینا کرشنن کر رہی ہیں، نے اس ہفتے کے شروع میں \"جرنل آف اپلائیڈ میتھس\" میں اپنی نتائج شائع کیے۔ ان کا دعویٰ ہے کہ ان کا الگورتھم موجودہ طریقوں کے مقابلے میں تقریباً 20-25% حساب کتاب کے وقت کو کم کرتا ہے۔ تصور کریں کہ کتنی پروسیسنگ پاور محفوظ ہو گئی!",
  "normalised": "ٹیم، جس کی قیادت پروفیسر مینا کرشنن کر رہی ہیں، نے اس ہفتے کے شروع میں \"جرنل آف اپلائیڈ میتھس\" میں اپنی نتائج شائع کیے۔ ان کا دعویٰ ہے کہ ان کا الگورتھم موجودہ طریقوں کے مقابلے میں تقریباً بیس سے پچیس فیصد حساب کتاب کے وقت کو کم کرتا ہے۔ تصور کریں کہ کتنی پروسیسنگ پاور محفوظ ہو گئی!",
  "raw": "ٹیم، جس کی قیادت پروفیسر مینا کرشنن کر رہی ہیں، نے اس ہفتے کے شروع میں \"جرنل آف اپلائیڈ میتھس\" میں اپنی نتائج شائع کیے۔ ان کا دعویٰ ہے کہ ان کا الگورتھم موجودہ طریقوں کے مقابلے میں تقریباً 20-25% حساب کتاب کے وقت کو کم کرتا ہے۔ تصور کریں کہ کتنی پروسیسنگ پاور محفوظ ہو گئی!\n\n{\n  \"unnormalised\": \"ٹیم، جس کی قیادت پروفیسر مینا کرشنن کر رہی ہیں، نے اس ہفتے کے شروع میں \\\"جرنل آف اپلائیڈ میتھس\\\" میں اپنی نتائج شائع کیے۔ ان کا دعویٰ ہے کہ ان کا الگورتھم موجودہ طریقوں کے مقابلے میں تقریباً 20-25% حساب کتاب کے وقت کو کم کرتا ہے۔ تصور کریں کہ کتنی پروسیسنگ پاور محفوظ ہو گئی!\",\n  \"normalised\": \"ٹیم، جس کی قیادت پروفیسر مینا کرشنن کر رہی ہیں، نے اس ہفتے کے شروع میں \\\"جرنل آف اپلائیڈ میتھس\\\" میں اپنی نتائج شائع کیے۔ ان کا دعویٰ ہے کہ ان کا الگورتھم موجودہ طریقوں کے مقابلے میں تقریباً بیس سے پچیس فیصد حساب کتاب کے وقت کو کم کرتا ہے۔ تصور کریں کہ کتنی پروسیسنگ پاور محفوظ ہو گئی!\"\n}"
}
```
```json
{
  "unnormalised": "تو لینیئر الجبرا کے ساتھ کیا بڑا معاملہ ہے؟ ٹھیک ہے، یہ بہت سی چیزوں کی ریڑھ کی ہڈی ہے جو ہم ہر روز استعمال کرتے ہیں، گوگل سرچ سے (الگوریتھم بنیادی طور پر لینیئر مساوات ہیں)، امیج پروسیسنگ سافٹ ویئر تک، یہاں تک کہ موسم کی پیش گوئی کے ماڈل تک۔ تمام شدید ریاضیاتی مساوات استعمال کرتے ہیں۔",
  "normalised": "تو لینیئر الجبرا کے ساتھ کیا بڑا معاملہ ہے؟ ٹھیک ہے، یہ بہت سی چیزوں کی ریڑھ کی ہڈی ہے جو ہم ہر روز استعمال کرتے ہیں، گوگل سرچ الگوریتھم سے بنیادی طور پر لینیئر مساوات ہیں، امیج پروسیسنگ سافٹ ویئر تک، یہاں تک کہ موسم کی پیش گوئی کے ماڈل تک۔ تمام شدید ریاضیاتی مساوات استعمال کرتے ہیں۔",
  "raw": "تو لینیئر الجبرا کے ساتھ کیا بڑا معاملہ ہے؟ ٹھیک ہے، یہ بہت سی چیزوں کی ریڑھ کی ہڈی ہے جو ہم ہر روز استعمال کرتے ہیں، گوگل سرچ سے (الگوریتھم بنیادی طور پر لینیئر مساوات ہیں)، امیج پروسیسنگ سافٹ ویئر تک، یہاں تک کہ موسم کی پیش گوئی کے ماڈل تک۔ تمام شدید ریاضیاتی مساوات استعمال کرتے ہیں۔"
}
```
```json
{
  "unnormalised": "پروفیسر کرشنن نے ذکر کیا کہ کوڈ گٹ ہب پر چوتھی سہ ماہی 2024 میں کسی وقت اوپن سورس کے طور پر جاری کیا جائے گا۔ جو بھی اس پر ہاتھ آزمانا چاہتا ہے وہ کر سکتا ہے۔ یہ بہت اچھا ہے!",
  "normalised": "پروفیسر کرشنن نے ذکر کیا کہ کوڈ جی-اِٹ-ایچ-یو-بی پر چوتھی سہ ماہی دو ہزار چوبیس میں کسی وقت اوپن سورس کے طور پر جاری کیا جائے گا۔ جو بھی اس پر ہاتھ آزمانا چاہتا ہے وہ کر سکتا ہے۔ یہ بہت اچھا ہے!",
  "raw": "پروفیسر کرشنن نے ذکر کیا کہ کوڈ گٹ ہب پر چوتھی سہ ماہی 2024 میں کسی وقت اوپن سورس کے طور پر جاری کیا جائے گا۔ جو بھی اس پر ہاتھ آزمانا چاہتا ہے وہ کر سکتا ہے۔ یہ بہت اچھا ہے!"
}
```
```json
{
  "unnormalised": "ابتدائی ٹیسٹ سے پتہ چلتا ہے کہ رفتار میں ایک اہم فرق ہے (تقریباً 0.25x تیز) جب اسے 2GB سے زیادہ کے ڈیٹا سیٹوں پر لاگو کیا جاتا ہے۔ کیا اس سے واقعی AI کی ترقی میں فرق پڑے گا؟ یہ تو وقت ہی بتائے گا۔",
  "normalised": "ابتدائی ٹیسٹ سے پتہ چلتا ہے کہ رفتار میں تقریباً صفر پوائنٹ دو پانچ x تیز رفتاری سے ایک اہم فرق ہے جب اسے دو گیگا بائٹس سے زیادہ کے ڈیٹا سیٹوں پر لاگو کیا جاتا ہے۔ کیا اس سے واقعی A-I کی ترقی میں فرق پڑے گا؟ یہ تو وقت ہی بتائے گا۔",
  "raw": "ابتدائی ٹیسٹ سے پتہ چلتا ہے کہ رفتار میں ایک اہم فرق ہے (تقریباً 0.25x تیز) جب اسے 2GB سے زیادہ کے ڈیٹا سیٹوں پر لاگو کیا جاتا ہے۔ کیا اس سے واقعی AI کی ترقی میں فرق پڑے گا؟ یہ تو وقت ہی بتائے گا۔\n\n{\n  \"unnormalised\": \"ابتدائی ٹیسٹ سے پتہ چلتا ہے کہ رفتار میں ایک اہم فرق ہے (تقریباً 0.25x تیز) جب اسے 2GB سے زیادہ کے ڈیٹا سیٹوں پر لاگو کیا جاتا ہے۔ کیا اس سے واقعی AI کی ترقی میں فرق پڑے گا؟ یہ تو وقت ہی بتائے گا۔\",\n  \"normalised\": \"ابتدائی ٹیسٹ سے پتہ چلتا ہے کہ رفتار میں تقریباً صفر پوائنٹ دو پانچ x تیز رفتاری سے ایک اہم فرق ہے جب اسے دو گیگا بائٹس سے زیادہ کے ڈیٹا سیٹوں پر لاگو کیا جاتا ہے۔ کیا اس سے واقعی A-I کی ترقی میں فرق پڑے گا؟ یہ تو وقت ہی بتائے گا۔\",\n  \"text\": \"ابتدائی ٹیسٹ سے پتہ چلتا ہے کہ رفتار میں ایک اہم فرق ہے (تقریباً 0.25x تیز) جب اسے 2GB سے زیادہ کے ڈیٹا سیٹوں پر لاگو کیا جاتا ہے۔ کیا اس سے واقعی AI کی ترقی میں فرق پڑے گا؟ یہ تو وقت ہی بتائے گا۔\"\n}"
}
```
