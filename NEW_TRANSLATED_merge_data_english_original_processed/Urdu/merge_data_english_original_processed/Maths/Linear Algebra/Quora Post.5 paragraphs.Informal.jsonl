```json
{
  "unnormalised": "ارے دوستو، لکیری الجبرا کے بارے میں ایک فوری سوال۔ میں ایک امتحان کی تیاری کر رہا ہوں اور کسی چیز پر پھنس گیا ہوں۔ میں کس طرح اصل میں ایگن ویلیوز اور ایگن ویکٹرز کو حقیقی دنیا کی چیزوں میں لاگو کروں، جیسے میٹرکس ٹرانسفارمیشن سے آگے؟ میری نصابی کتاب میں صرف نظریاتی مثالیں ہیں، لیکن میں یہ جاننا چاہتا ہوں کہ یہ چیزیں کہاں کارآمد ہیں، جانتے ہو؟ سوچیں جیسے، کیا یہ سگنل پروسیسنگ میں یا شاید AI اسٹف میں بھی استعمال ہوتا ہے؟ کوئی بھی آسان وضاحت یا مثالیں بہت مددگار ہوں گی!",
  "normalised": "ارے دوستو، لکیری الجبرا کے بارے میں ایک فوری سوال۔ میں ایک امتحان کی تیاری کر رہا ہوں اور کسی چیز پر پھنس گیا ہوں۔ میں کس طرح اصل میں ایگن ویلیوز اور ایگن ویکٹرز کو حقیقی دنیا کی چیزوں میں لاگو کروں، جیسے میٹرکس ٹرانسفارمیشن سے آگے؟ میری نصابی کتاب میں صرف نظریاتی مثالیں ہیں، لیکن میں یہ جاننا چاہتا ہوں کہ یہ چیزیں کہاں کارآمد ہیں، جانتے ہو؟ سوچیں جیسے، کیا یہ سگنل پروسیسنگ میں یا شاید اے آئی اسٹف میں بھی استعمال ہوتا ہے؟ کوئی بھی آسان وضاحت یا مثالیں بہت مددگار ہوں گی!",
  "raw": "ارے دوستو، لکیری الجبرا کے بارے میں ایک فوری سوال۔ میں ایک امتحان کی تیاری کر رہا ہوں اور کسی چیز پر پھنس گیا ہوں۔ میں کس طرح اصل میں ایگن ویلیوز اور ایگن ویکٹرز کو حقیقی دنیا کی چیزوں میں لاگو کروں، جیسے میٹرکس ٹرانسفارمیشن سے آگے؟ میری نصابی کتاب میں صرف نظریاتی مثالیں ہیں، لیکن میں یہ جاننا چاہتا ہوں کہ یہ چیزیں کہاں کارآمد ہیں، جانتے ہو؟ سوچیں جیسے، کیا یہ سگنل پروسیسنگ میں یا شاید AI اسٹف میں بھی استعمال ہوتا ہے؟ کوئی بھی آسان وضاحت یا مثالیں بہت مددگار ہوں گی!"
}
```
```json
{
  "unnormalised": "تو، مجھے ہمارے پروفیسر، ڈاکٹر ورما، کا ذکر یاد ہے کہ انہوں نے پیج رینک الگورتھم میں ان کے استعمال کے بارے میں کچھ ذکر کیا تھا، جو کہ گوگل کا اصل الگورتھم تھا۔ مجھے لگتا ہے کہ یہ ایک بہت بڑے میٹرکس کے eigenvectors پر مبنی تھا جو ویب کے لنک ڈھانچے کی نمائندگی کرتا ہے۔ سب سے بڑی eigenvalue کے ساتھ eigenvector آپ کو ہر صفحہ کے لئے اہمیت کا سکور دیتا ہے۔ بہت عمدہ، ہے نا؟ تقریباً جادو کی طرح محسوس ہوتا ہے، ہاہا۔ آپ کو حیرت ہوتی ہے کہ لکیری الجبرا میں اور کون سی عمدہ ترکیبیں چھپی ہوئی ہیں۔",
  "normalised": "تو، مجھے ہمارے پروفیسر، ڈاکٹر ورما، کا ذکر یاد ہے کہ انہوں نے پیج رینک الگورتھم میں ان کے استعمال کے بارے میں کچھ ذکر کیا تھا، جو کہ گوگل کا اصل الگورتھم تھا۔ مجھے لگتا ہے کہ یہ ایک بہت بڑے میٹرکس کے eigenvectors پر مبنی تھا جو ویب کے لنک ڈھانچے کی نمائندگی کرتا ہے۔ سب سے بڑی eigenvalue کے ساتھ eigenvector آپ کو ہر صفحہ کے لئے اہمیت کا سکور دیتا ہے۔ بہت عمدہ، ہے نا؟ تقریباً جادو کی طرح محسوس ہوتا ہے، ہا ہا۔ آپ کو حیرت ہوتی ہے کہ لکیری الجبرا میں اور کون سی عمدہ ترکیبیں چھپی ہوئی ہیں۔",
  "raw": "تو، مجھے ہمارے پروفیسر، ڈاکٹر ورما، کا ذکر یاد ہے کہ انہوں نے پیج رینک الگورتھم میں ان کے استعمال کے بارے میں کچھ ذکر کیا تھا، جو کہ گوگل کا اصل الگورتھم تھا۔ مجھے لگتا ہے کہ یہ ایک بہت بڑے میٹرکس کے eigenvectors پر مبنی تھا جو ویب کے لنک ڈھانچے کی نمائندگی کرتا ہے۔ سب سے بڑی eigenvalue کے ساتھ eigenvector آپ کو ہر صفحہ کے لئے اہمیت کا سکور دیتا ہے۔ بہت عمدہ، ہے نا؟ تقریباً جادو کی طرح محسوس ہوتا ہے، ہاہا۔ آپ کو حیرت ہوتی ہے کہ لکیری الجبرا میں اور کون سی عمدہ ترکیبیں چھپی ہوئی ہیں۔"
}
```
```json
{
  "unnormalised": "ایک اور ایپلیکیشن جو میں نے کہیں دیکھی وہ انجینئرنگ سسٹمز میں استحکام کے تجزیہ سے متعلق تھی۔ جیسے، اگر آپ کے پاس تفریق مساوات کے ذریعہ بیان کردہ کوئی نظام ہے، تو آپ نظام کے میٹرکس کے eigenvalues کا استعمال یہ تعین کرنے کے لئے کر سکتے ہیں کہ آیا نظام مستحکم ہے یا نہیں۔ اگر تمام eigenvalues کے منفی حقیقی حصے ہیں، تو نظام مستحکم ہے۔ اگر آپ capacitors (C= 10μF)، inductors (L = 5mH)، اور resistors (R=10 Ohms) کے ساتھ برقی سرکٹس ماڈل بنانا چاہتے ہیں، تو eigenvalues تلاش کرنے سے آپ کو یہ سمجھنے میں مدد ملتی ہے کہ سرکٹ oscillates ہوتا ہے یا decay ہوتا ہے۔",
  "normalised": "ایک اور ایپلیکیشن جو میں نے کہیں دیکھی وہ انجینئرنگ سسٹمز میں استحکام کے تجزیہ سے متعلق تھی۔ جیسے، اگر آپ کے پاس تفریق مساوات کے ذریعہ بیان کردہ کوئی نظام ہے، تو آپ نظام کے میٹرکس کے eigenvalues کا استعمال یہ تعین کرنے کے لئے کر سکتے ہیں کہ آیا نظام مستحکم ہے یا نہیں۔ اگر تمام eigenvalues کے منفی حقیقی حصے ہیں، تو نظام مستحکم ہے۔ اگر آپ capacitors (C دس مائیکرو فاراد کے برابر)، inductors (L پانچ ملی ہینری کے برابر)، اور resistors (R دس Ohms کے برابر) کے ساتھ برقی سرکٹس ماڈل بنانا چاہتے ہیں، تو eigenvalues تلاش کرنے سے آپ کو یہ سمجھنے میں مدد ملتی ہے کہ سرکٹ oscillates ہوتا ہے یا decay ہوتا ہے۔",
  "raw": "ایک اور ایپلیکیشن جو میں نے کہیں دیکھی وہ انجینئرنگ سسٹمز میں استحکام کے تجزیہ سے متعلق تھی۔ جیسے، اگر آپ کے پاس تفریق مساوات کے ذریعہ بیان کردہ کوئی نظام ہے، تو آپ نظام کے میٹرکس کے eigenvalues کا استعمال یہ تعین کرنے کے لئے کر سکتے ہیں کہ آیا نظام مستحکم ہے یا نہیں۔ اگر تمام eigenvalues کے منفی حقیقی حصے ہیں، تو نظام مستحکم ہے۔ اگر آپ capacitors (C= 10μF)، inductors (L = 5mH)، اور resistors (R=10 Ohms) کے ساتھ برقی سرکٹس ماڈل بنانا چاہتے ہیں، تو eigenvalues تلاش کرنے سے آپ کو یہ سمجھنے میں مدد ملتی ہے کہ سرکٹ oscillates ہوتا ہے یا decay ہوتا ہے۔"
}
```
```json
{
  "unnormalised": "اور ہاں، بالکل اے آئی۔ Eigenvalues اور eigenvectors dimensionality reduction کی تکنیک جیسے Principal Component Analysis (PCA) میں ایک بڑا کردار ادا کرتے ہیں۔ تصور کریں کہ آپ ٹنوں خصوصیات والے ڈیٹا سیٹ کے ساتھ کام کر رہے ہیں، جیسے چہروں کی تصاویر کا ایک ڈیٹا سیٹ۔ ہر تصویر میں 1000 خصوصیات (1000 پکسلز) ہیں، آپ PCA کا استعمال \"اصولی اجزاء\" تلاش کرنے کے لیے کر سکتے ہیں، جو بنیادی طور پر آپ کے ڈیٹا کے کوویرینس میٹرکس کے سب سے بڑے eigenvalues کے مساوی eigenvectors ہیں۔ آپ اپنے ڈیٹا کی نمائندگی کرنے اور طول و عرض کو ½ میں کاٹنے کے لیے ان چند اجزاء کا استعمال کر سکتے ہیں۔ پھر ماڈل بنانا آسان ہے (مثال کے طور پر، تصویر کی شناخت میں)۔",
  "normalised": "اور ہاں، بالکل اے-آئی۔ Eigenvalues اور eigenvectors dimensionality reduction کی تکنیک جیسے Principal Component Analysis (P-C-A) میں ایک بڑا کردار ادا کرتے ہیں۔ تصور کریں کہ آپ ٹنوں خصوصیات والے ڈیٹا سیٹ کے ساتھ کام کر رہے ہیں، جیسے چہروں کی تصاویر کا ایک ڈیٹا سیٹ۔ ہر تصویر میں ایک ہزار خصوصیات (ایک ہزار پکسلز) ہیں، آپ P-C-A کا استعمال \"اصولی اجزاء\" تلاش کرنے کے لیے کر سکتے ہیں، جو بنیادی طور پر آپ کے ڈیٹا کے کوویرینس میٹرکس کے سب سے بڑے eigenvalues کے مساوی eigenvectors ہیں۔ آپ اپنے ڈیٹا کی نمائندگی کرنے اور طول و عرض کو ایک تقسیم دو میں کاٹنے کے لیے ان چند اجزاء کا استعمال کر سکتے ہیں۔ پھر ماڈل بنانا آسان ہے (مثال کے طور پر، تصویر کی شناخت میں)۔",
  "raw": "اور ہاں، بالکل اے آئی۔ Eigenvalues اور eigenvectors dimensionality reduction کی تکنیک جیسے Principal Component Analysis (PCA) میں ایک بڑا کردار ادا کرتے ہیں۔ تصور کریں کہ آپ ٹنوں خصوصیات والے ڈیٹا سیٹ کے ساتھ کام کر رہے ہیں، جیسے چہروں کی تصاویر کا ایک ڈیٹا سیٹ۔ ہر تصویر میں 1000 خصوصیات (1000 پکسلز) ہیں، آپ PCA کا استعمال \"اصولی اجزاء\" تلاش کرنے کے لیے کر سکتے ہیں، جو بنیادی طور پر آپ کے ڈیٹا کے کوویرینس میٹرکس کے سب سے بڑے eigenvalues کے مساوی eigenvectors ہیں۔ آپ اپنے ڈیٹا کی نمائندگی کرنے اور طول و عرض کو ½ میں کاٹنے کے لیے ان چند اجزاء کا استعمال کر سکتے ہیں۔ پھر ماڈل بنانا آسان ہے (مثال کے طور پر، تصویر کی شناخت میں)۔"
}
```
```json
{
  "unnormalised": "امید ہے کہ اس سے تھوڑی مدد ملے گی! لکیری الجبرا بعض اوقات مبہم لگ سکتا ہے، لیکن eigenvalues اور eigenvectors کی بہت سی ایپلی کیشنز موجود ہیں، اگر آپ صرف ان کی تلاش کریں۔ اور مت بھولنا، مشق کلید ہے! آپ کے امتحان کے ساتھ گڈ لک برو!",
  "normalised": "امید ہے کہ اس سے تھوڑی مدد ملے گی! لکیری الجبرا بعض اوقات مبہم لگ سکتا ہے، لیکن eigenvalues اور eigenvectors کی بہت سی ایپلی کیشنز موجود ہیں، اگر آپ صرف ان کی تلاش کریں۔ اور مت بھولو، مشق کلید ہے! آپ کے امتحان کے ساتھ گڈ لک برو!",
  "raw": "امید ہے کہ اس سے تھوڑی مدد ملے گی! لکیری الجبرا بعض اوقات مبہم لگ سکتا ہے، لیکن eigenvalues اور eigenvectors کی بہت سی ایپلی کیشنز موجود ہیں، اگر آپ صرف ان کی تلاش کریں۔ اور مت بھولنا، مشق کلید ہے! آپ کے امتحان کے ساتھ گڈ لک برو!"
}
```
