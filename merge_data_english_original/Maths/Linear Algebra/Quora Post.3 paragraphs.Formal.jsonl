{"unnormalised": "Someone asked me about the applications of Linear Algebra in Machine Learning. Well, it's pretty fundamental. Consider a basic neural network layer; itâ€™s essentially a matrix multiplication. Each neuron's output is a weighted sum of its inputs, and these weights are organized into matrices. For example, a layer might have weights W with dimensions (100 x 50), meaning it transforms a 50-dimensional input into a 100-dimensional output. The forward pass involves calculating Wx + b, where x is the input vector and b is a bias vector. This 'Wx' is precisely where linear algebra shines.", "normalised": "Someone asked me about the applications of Linear Algebra in Machine Learning. Well, it's pretty fundamental. Consider a basic neural network layer; it's essentially a matrix multiplication. Each neuron's output is a weighted sum of its inputs, and these weights are organized into matrices. For example, a layer might have weights W with dimensions one hundred by fifty, meaning it transforms a fifty-dimensional input into a one hundred-dimensional output. The forward pass involves calculating W x plus b, where x is the input vector and b is a bias vector. This 'W x' is precisely where linear algebra shines."}
{"unnormalised": "Then you have image processing. An image can be represented as a matrix (or a set of matrices for color images). Operations like blurring, edge detection, and rotations all rely on linear algebra concepts. For instance, applying a 3x3 convolution kernel involves element-wise multiplication and summation which can be represented using matrix operations. Even simple tasks like resizing an image or cropping it can be viewed as linear transformations. Image compression techniques such as PCA also heavily use linear algebra.", "normalised": "Then you have image processing. An image can be represented as a matrix or a set of matrices for color images. Operations like blurring, edge detection, and rotations all rely on linear algebra concepts. For instance, applying a three by three convolution kernel involves element-wise multiplication and summation which can be represented using matrix operations. Even simple tasks like resizing an image or cropping it can be viewed as linear transformations. Image compression techniques such as P-C-A also heavily use linear algebra."}
{"unnormalised": "Further, Linear Algebra underpins many optimization algorithms used in Machine Learning like Gradient Descent. These algorithms are crucial for training models efficiently. So, while you might not be explicitly writing out matrices all the time, a strong understanding of linear algebra will definitely help you understand the inner workings of many ML models, and also help you with debugging your code. You'll encounter things like eigenvalues, eigenvectors, matrix decomposition, and vector spaces all the time.", "normalised": "Further, Linear Algebra underpins many optimization algorithms used in Machine Learning like Gradient Descent. These algorithms are crucial for training models efficiently. So, while you might not be explicitly writing out matrices all the time, a strong understanding of linear algebra will definitely help you understand the inner workings of many M-L models, and also help you with debugging your code. You'll encounter things like eigenvalues, eigenvectors, matrix decomposition, and vector spaces all the time."}
