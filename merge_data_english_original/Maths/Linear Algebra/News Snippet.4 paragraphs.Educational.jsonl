{"unnormalised": "IIT Delhi researchers have achieved a breakthrough in optimizing Linear Algebra calculations, potentially revolutionizing fields like Machine Learning. The team, led by Prof. A.K. Sharma, developed a novel algorithm that significantly reduces computational complexity.", "normalised": "I-I-T Delhi researchers have achieved a breakthrough in optimizing Linear Algebra calculations, potentially revolutionizing fields like Machine Learning. The team, led by Professor A. K. Sharma, developed a novel algorithm that significantly reduces computational complexity."}
{"unnormalised": "The new method focuses on solving systems of linear equations, a fundamental operation represented as Ax = b. Initial tests showed a reduction in computations by approximately 25% for matrices of size 1000x1000, potentially scaling to larger matrices.", "normalised": "The new method focuses on solving systems of linear equations, a fundamental operation represented as A x equals b. Initial tests showed a reduction in computations by approximately twenty five percent for matrices of size one thousand by one thousand, potentially scaling to larger matrices."}
{"unnormalised": "This advancement has significant implications for various sectors. Industries relying heavily on data analysis, such as finance where algorithms process complex datasets involving transactions worth â‚¹100000000s daily, and engineering, are poised to benefit immensely. The algorithm could drastically speed up simulation times & data processing.", "normalised": "This advancement has significant implications for various sectors. Industries relying heavily on data analysis, such as finance where algorithms process complex datasets involving transactions worth rupees one hundred millions daily, and engineering, are poised to benefit immensely. The algorithm could drastically speed up simulation times and data processing."}
{"unnormalised": "Prof. Sharma believes the new algorithm could also enhance AI model training. By optimizing matrix operations, training times for complex models (like those used in facial recognition or natural language processing) could be cut down from 72 hrs to approx. 50 hrs, leading to faster development cycles. Further research is underway to explore its applications in quantum computing, leveraging matrices with dimensions reaching sizes of 2048 x 2048.", "normalised": "Professor Sharma believes the new algorithm could also enhance A-I model training. By optimizing matrix operations, training times for complex models like those used in facial recognition or natural language processing could be cut down from seventy two hours to approximately fifty hours, leading to faster development cycles. Further research is underway to explore its applications in quantum computing, leveraging matrices with dimensions reaching sizes of two thousand and forty eight by two thousand and forty eight."}
